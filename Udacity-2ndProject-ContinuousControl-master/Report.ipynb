{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for Continous_Control Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Shengjie Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will work with the Reacher environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"reacher.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, a double-jointed arm can move to target locations. A reward of 0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. There are four entries in the action vector, each entry should be a number between -1 and 1.\n",
    "\n",
    "This task is episodic, and in order to solve this environment, our agent needs to achieve an average score of +30 over 100 consecutive episodes.  \n",
    "\n",
    "We will use reinforcement learning methods, specifically, DDPG algorithm (deep deterministic policy graident), to learn a suitable policy in a model-free reinforcement learning setting using a Unity environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG algorithm uses four neural networks which include critic network, target critic network, deterministic policy network and target policy network. The critic network here is a function approximator for the q value function which has state and action as the input; the deterministic policy network here is a function approximator for the policy function which has state as the input; their target networks have same network architectures but their parameters will be updated by using soft-update.  \n",
    "\n",
    "For our problem, actor has the follwing architecture: $${fc}_1: Linear(state size, 1028)$$ $${fc}_2: Linear(1028, 256)$$ $${fc}_3: Linear(256, action size)$$ and we will use relu function as our activation function for the fully-collected layers and tanh function for the output layer since our action belongs to $[-1,1]$; critic has the following architecture: $${fc}_1: Linear(state size + action size, 1028)$$ $${fc}_2: Linear(1028, 256)$$ $${fc}_3: Linear(256, 1)$$ and we will use relu function as activation function for the fully connected layers and no activation for output layers since the action values belong to real number.  \n",
    "\n",
    "We will also use replaybuffer to store experienced tuple and sample from it since it can decorrelate the experenced tuple to stablize the trainining. For each episode, we use local actor network to decide our action based on the current state and add a normal noise to it for action exploration, after we observe reward and next state we store them in the replaybuffer and then sample some tuples to do learning for parameter updating.  \n",
    "\n",
    "Note this is a off-policy learning, for the critic network, we will use one-step bootstrapping estimate ($r_i + \\gamma Q_{\\text{target}}(s_{i+1}, \\mu^{'}(s_{i+1}, ))$) as our TD target with MSE loss to update the parameters for the critic network; for the policy network, we will directly approximately maximize the expected action value by using sampled policy gradient, which is $$\\triangledown_{\\theta_\\mu}J \\approx \\frac{1}{N}\\sum_{i}\\triangledown_{a}Q(s, a)|_{s = s_i, a = \\mu(s_i)}\\triangledown_{\\theta_\\mu}\\mu(s)|s_i$$  \n",
    "\n",
    "Then, we will update the target network by using soft update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BUFFER_SIZE`: 10000 (max size for the replay buffer)  \n",
    "\n",
    "`BATCH_SIZE`: 64 (training batch size)\n",
    "\n",
    "`ALPHA`: 1e-2 (soft-update rate)  \n",
    "\n",
    "`UPDATE_EVERY`: 20  (update parameter for every # step)  \n",
    "\n",
    "`GAMMA`: 0.99  (Discount factor)  \n",
    "\n",
    "`NOISE`: 1e-3  (Noise for action exploration)  \n",
    "\n",
    "`NUM_UPDATES`: 10 (number of updates per update step)  \n",
    "\n",
    "`LR_actor`:  1e-4 (learning rate for actor network)  \n",
    "\n",
    "`LR_critic`: 1e-3 (learning rate for critic network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"average_score.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can see that our agent solves the environment in the 205 episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"reward.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the average score against the episode number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to use better noise setting for the action exploration. Besides, we will try to use other algorithms such as \n",
    "PPO, TRPO and etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
