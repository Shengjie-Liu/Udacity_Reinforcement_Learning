{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for Navigation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Shengjie Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will train an agent to navigate (and collect bananas!) in a large, square world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pasted Graphic.png\">\n",
    "\\begin{center}\n",
    "\t\\includegraphics[scale=0.4]{Pasted Graphic.png}\n",
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana. Thus, the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.\n",
    "\n",
    "The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around the agent's forward direction. Given this information, the agent has to learn how to best select actions. Four discrete actions are available, corresponding to:  \n",
    "\n",
    "-`0`: move forward\n",
    "-`1`: move backward\n",
    "-`2`: turn left\n",
    "-`3`: turn right  \n",
    "    \n",
    "We will use value-based reinforcement learning methods, specifically, Deep Q-learning and its enhancements, to learn a suitable policy in a model-free reinforcement learning setting using a Unity environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) The first learning algorithm is the deep Q learning network, which is an off-policy algorithm where the policy being \n",
    "evaluated is not same with the policy being learned.  \n",
    "\n",
    "This algorithm will use two identical neural network (local network and target network) as the function approximator to learn the function which maps the state to every action values. The weight of local network will be learned through samples and the weight of target network will be updated periodically by copying from local network and then making a weighted combination between its original weights and local network weights.  \n",
    "\n",
    "Through above two networks, this becomes a supervised learning problem where $\\hat{Q}_{\\text{local}}(s_t, a_t; \\theta)$ serves as the expected value and $R_t + \\gamma*\\max(\\hat{Q}_{\\text{target}}(s_{t+1}))$ becomes the target. Then, it is natural to use MSE as the loss function and update the weights accordingly.  \n",
    "\n",
    "For the function approximator Q network, we choose a $3$ fully connected hidden layers and $1$ linear output layer with the size defined as $$\\text{hidden_layer_}1: (\\text{state_size}, 1028)$$ $$\\text{hidden_layer_}2: (1028, 256)$$ $$\\text{hidden_layer_}3: (256, 128)$$ $$\\text{output_layer}: (128, \\text{action_size})$$ We apply relu activation after each full-connected layer. Adam optimizer is used as the optimizer for finding the optimal weights. The weight iteration equation is: $$W_{t+1} = W_t + \\text{learning_rate}*[R_t + \\gamma\\max\\hat{Q}_{\\text{target}}(S_{t+1}) - \\hat{Q}_{\\text{local}}(S_t, A_t)]\\triangledown_{W} \\hat{Q}_{\\text{local}}(S_t, A_t)$$\n",
    "Since this algorithm has the overestimation problem for the action value, it is highly unstable. Then, we use double DQN to reduce this problem.  \n",
    "\n",
    "(2) In order to solve overestimation action value problem, we apply double DQN network which doesn't change too much compared with fixed Q target DQN. Only one change to the weight update process $$W_{t+1} = W_t + \\text{learning_rate}[R_t + \\gamma\\hat{Q}_{\\text{target}}(S_t, \\text{argmax}_a\\hat{Q}_{\\text{local}}(S_t, a))- \\hat{Q}_{\\text{local}}(S_t, A_t)]\\triangledown_{W} \\hat{Q}_{\\text{local}}(S_t, A_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-`BUFFER_SIZE`: 1e5 (replay buffer size)  \n",
    "\n",
    "-`BATCH_SIZE`: 64 (train batch size)  \n",
    "\n",
    "-`EPSILON`: 0.05 (epsilon-greedy policy)  \n",
    "\n",
    "-`ALPHA`: 1e-3 (soft-update rate)  \n",
    "\n",
    "-`LR`: 5e-4 (learning rate)  \n",
    "\n",
    "-`UPDATE_EVERY`: 4  (target network parameter change period)  \n",
    "\n",
    "-`GAMMA`: 0.99 (discount factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Deep Q network with fixed Q target, we have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Deep Q network.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It solves the problem in 300 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the double deep Q network with fixed Q target, we have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Double Deep Q network.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It solves the problem in 300 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with these two networks, we can see that Double Deep Q network has a better convergence result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try to combine Double DQN with Prioritized Replay Buffer to get a better result. Besides, for the pixel input environment,\n",
    "we could also apply Dueling Deep Q network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
